{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 210070, number of negative: 210286\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.483221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3560\n",
      "[LightGBM] [Info] Number of data points in the train set: 420356, number of used features: 717\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 210071, number of negative: 210286\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.021301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3562\n",
      "[LightGBM] [Info] Number of data points in the train set: 420357, number of used features: 717\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 210071, number of negative: 210286\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.005001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3562\n",
      "[LightGBM] [Info] Number of data points in the train set: 420357, number of used features: 717\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 210070, number of negative: 210287\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.293735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3562\n",
      "[LightGBM] [Info] Number of data points in the train set: 420357, number of used features: 717\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 210070, number of negative: 210287\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.127511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3563\n",
      "[LightGBM] [Info] Number of data points in the train set: 420357, number of used features: 717\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Precision: 0.9985363845649555\n",
      "Recall: 0.995122766500547\n",
      "ROC AUC Score: 0.9999022535022555\n",
      "Average Precision Score: 0.9999109563495034\n",
      "F1 Score: 0.9968266530702322\n",
      "Submission file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, average_precision_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Load the datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(df):\n",
    "    # Convert categorical variables into dummy/indicator variables\n",
    "    df = pd.get_dummies(df, columns=['category', 'gender', 'state', 'job'])\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    df['trans_date'] = pd.to_datetime(df['trans_date'])\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    \n",
    "    # Extract useful features from date columns\n",
    "    df['trans_year'] = df['trans_date'].dt.year\n",
    "    df['trans_month'] = df['trans_date'].dt.month\n",
    "    df['trans_day'] = df['trans_date'].dt.day\n",
    "    df['dob_year'] = df['dob'].dt.year\n",
    "    df['dob_month'] = df['dob'].dt.month\n",
    "    df['dob_day'] = df['dob'].dt.day\n",
    "    \n",
    "    # Create interaction features\n",
    "    df['amt_trans_hour'] = df['amt'] * pd.to_datetime(df['trans_time'], format='%H:%M:%S').dt.hour\n",
    "    df['age_amt'] = (2023 - df['dob'].dt.year) * df['amt']\n",
    "    \n",
    "    # Binning continuous features\n",
    "    df['amt_bin'] = pd.cut(df['amt'], bins=10, labels=False)\n",
    "    df['age_bin'] = pd.cut(2023 - df['dob'].dt.year, bins=10, labels=False)\n",
    "    \n",
    "    # Drop columns that are not useful for prediction\n",
    "    df.drop(['trans_num', 'trans_date', 'trans_time', 'unix_time', 'cc_num', 'first', 'last', 'street', 'city', 'zip', 'lat', 'long', 'dob', 'merchant', 'merch_lat', 'merch_long'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = preprocess_data(train)\n",
    "test = preprocess_data(test)\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X = train.drop('is_fraud', axis=1)\n",
    "y = train['is_fraud']\n",
    "\n",
    "# Address class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "# Define the LightGBM model with class weight adjustment and early stopping\n",
    "model = lgb.LGBMClassifier(\n",
    "    random_state=42, \n",
    "    learning_rate=0.1, \n",
    "    n_estimators=2000, \n",
    "    num_leaves=31, \n",
    "    max_depth=-1, \n",
    "    min_child_samples=20,\n",
    "    reg_alpha=0.1,  # L1 regularization\n",
    "    reg_lambda=0.1,  # L2 regularization\n",
    "    class_weight='balanced',  # Adjust class weights\n",
    ")\n",
    "\n",
    "# Train the model with stratified K-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, val_index in skf.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], eval_metric='binary_logloss')\n",
    "\n",
    "# Make predictions on the validation set with probability threshold adjustment\n",
    "y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "optimal_threshold = 0.5  # You can adjust this threshold based on your validation set performance\n",
    "y_val_pred = (y_val_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Evaluate the model using various metrics for imbalanced data\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred)\n",
    "recall = recall_score(y_val, y_val_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "average_precision = average_precision_score(y_val, y_val_pred_proba)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n",
    "print(f\"Average Precision Score: {average_precision}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Make predictions on the test set with probability threshold adjustment\n",
    "test_predictions_proba = model.predict_proba(test)[:, 1]\n",
    "test_predictions = (test_predictions_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Create a submission file\n",
    "submission = sample_submission.copy()\n",
    "submission['is_fraud'] = test_predictions\n",
    "\n",
    "submission.to_csv('submission16.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
